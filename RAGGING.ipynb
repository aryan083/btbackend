{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwFyo8asXIR_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtCntx7zQKJE"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJX7lKwwQQ6p"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "def count_text_tokens(text):\n",
        "    \"\"\"Counts the number of tokens in a text string based on word count.\"\"\"\n",
        "    # A simple word split to approximate token count\n",
        "    words = text.split()\n",
        "    return len(words)\n",
        "\n",
        "def extract_text_from_pdf(pdf_file_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_file_path)\n",
        "        text = \"\"\n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            text += page.get_text(\"text\")  # Extract text from each page\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def count_pdf_tokens(pdf_file_path):\n",
        "    \"\"\"Counts tokens in a PDF file.\"\"\"\n",
        "    text = extract_text_from_pdf(pdf_file_path)\n",
        "    return count_text_tokens(text)\n",
        "\n",
        "def count_tokens_in_pdf_directory(directory_path):\n",
        "    \"\"\"Counts total tokens for all PDF files in a directory.\"\"\"\n",
        "    total_tokens = 0\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            print(f\"Processing file: {filename}\")\n",
        "            file_tokens = count_pdf_tokens(file_path)\n",
        "            total_tokens += file_tokens\n",
        "            print(f\"Tokens for {filename}: {file_tokens}\")\n",
        "\n",
        "    print(f\"Total tokens for all files in the directory: {total_tokens}\")\n",
        "    return total_tokens\n",
        "\n",
        "# Example usage:\n",
        "directory_path = \"/content/Data/\"\n",
        "total_tokens = count_tokens_in_pdf_directory(directory_path)+\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RLQq7-uxo5jk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "\n",
        "def count_text_tokens(text):\n",
        "    \"\"\"Counts the number of tokens in a text string based on word count.\"\"\"\n",
        "    # A simple word split to approximate token count\n",
        "    words = text.split()\n",
        "    return len(words)\n",
        "\n",
        "def estimate_base64_image_tokens(base64_string):\n",
        "    \"\"\"Estimates token cost for a base64-encoded image.\"\"\"\n",
        "    try:\n",
        "        decoded_image = base64.b64decode(base64_string)\n",
        "        image_size_bytes = len(decoded_image)\n",
        "        tokens = image_size_bytes / 50  # Adjust the divisor as needed\n",
        "        return int(tokens)\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding base64: {e}\")\n",
        "        return 0\n",
        "\n",
        "def count_json_tokens(json_file_path, include_images=False):\n",
        "    \"\"\"Counts tokens in a JSON file, optionally including base64 images.\"\"\"\n",
        "    try:\n",
        "        with open(json_file_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Convert the entire JSON content to a string and count tokens\n",
        "        json_string = json.dumps(data)\n",
        "        json_tokens = count_text_tokens(json_string)\n",
        "\n",
        "        image_tokens = 0\n",
        "        if include_images:\n",
        "            if isinstance(data, dict):\n",
        "                for value in data.values():\n",
        "                    if isinstance(value, str) and value.startswith(\"data:image/\"):\n",
        "                        image_tokens += estimate_base64_image_tokens(value.split(',')[1])\n",
        "            elif isinstance(data, list):\n",
        "                for item in data:\n",
        "                    if isinstance(item, dict):\n",
        "                        for value in item.values():\n",
        "                            if isinstance(value, str) and value.startswith(\"data:image/\"):\n",
        "                                image_tokens += estimate_base64_image_tokens(value.split(',')[1])\n",
        "\n",
        "        return json_tokens + image_tokens\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found: {json_file_path}\")\n",
        "        return 0\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON in file: {json_file_path}\")\n",
        "        return 0\n",
        "\n",
        "def count_tokens_in_directory(directory_path, include_images=False):\n",
        "    \"\"\"Counts total tokens for all JSON files in a directory.\"\"\"\n",
        "    total_tokens = 0\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".json\"):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            print(f\"Processing file: {filename}\")\n",
        "            file_tokens = count_json_tokens(file_path, include_images)\n",
        "            total_tokens += file_tokens\n",
        "            print(f\"Tokens for {filename}: {file_tokens}\")\n",
        "\n",
        "    print(f\"Total tokens for all files in the directory: {total_tokens}\")\n",
        "    return total_tokens\n",
        "\n",
        "# Example usage:\n",
        "directory_path = \"/content/Data/Book1/Text\"\n",
        "directory_path2 = \"/content/Data/Book1/Img\"\n",
        "\n",
        "include_images = True  # Set to False if you don't want to include base64 images\n",
        "\n",
        "total_tokens =  count_tokens_in_directory(directory_path, include_images) + count_tokens_in_directory(directory_path2, include_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OWd6EAWwwRxe"
      },
      "outputs": [],
      "source": [
        "#START OF RAG GIG NO GAG\n",
        "!pip install langchain supabase sentence-transformers torch google-generativeai python-dotenv tqdm numpy multipart torch pip  langchain-community  InstructorEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C91NPka65Ab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Any\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from supabase import create_client, Client\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iucs-ErT67uN"
      },
      "outputs": [],
      "source": [
        "# Initialize with Colab secrets\n",
        "SUPABASE_URL = userdata.get('SUPABASE_URL')\n",
        "SUPABASE_KEY = userdata.get('SUPABASE_ANON_KEY')\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LsPZFV-9yr6m",
        "outputId": "b4bec6e2-c571-4992-db15-d4dc49e4f046"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Text summary error: HTTPConnectionPool(host='localhost', port=36159): Read timed out. (read timeout=600.0)\n",
            "ERROR:__main__:Text summary error: HTTPConnectionPool(host='localhost', port=36159): Read timed out. (read timeout=600.0)\n",
            "ERROR:__main__:Text summary error: HTTPConnectionPool(host='localhost', port=36159): Read timed out. (read timeout=600.0)\n",
            "ERROR:__main__:Text summary error: HTTPConnectionPool(host='localhost', port=36159): Read timed out. (read timeout=600.0)\n",
            "ERROR:__main__:Text summary error: HTTPConnectionPool(host='localhost', port=36159): Read timed out. (read timeout=600.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Any, List\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from supabase import create_client, Client\n",
        "import uuid\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ---------------------------- Interfaces ----------------------------\n",
        "class SummaryGenerator(ABC):\n",
        "    @abstractmethod\n",
        "    def generate_text_summary(self, text: str) -> Optional[str]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate_image_summary(self, image_path: str) -> Optional[str]:\n",
        "        pass\n",
        "\n",
        "class EmbeddingGenerator(ABC):\n",
        "    @abstractmethod\n",
        "    def generate_embedding(self, text: str) -> Optional[list]:\n",
        "        pass\n",
        "\n",
        "class ContentProcessor(ABC):\n",
        "    @abstractmethod\n",
        "    def process(self, base_path: str, course_id: str) -> None:\n",
        "        pass\n",
        "\n",
        "# ---------------------------- Implementations ----------------------------\n",
        "class GeminiSummaryGenerator(SummaryGenerator):\n",
        "    def __init__(self, api_key: str):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.text_model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "        self.vision_model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "    def generate_text_summary(self, text: str) -> Optional[str]:\n",
        "        try:\n",
        "            response = self.text_model.generate_content(\n",
        "                f\"Generate a highly detailed and long summary of the given text, ensuring that each response reaches the maximum possible token limit of 8192 tokens without exceeding it. The complete summary must contain a minimum of 32,700 words. If necessary, generate the summary in coherent, self-contained chunks while maintaining logical flow between sections. Avoid unnecessary elaboration or repetition. Directly provide the summary without any introductory phrases, headings, or indications like Here is the summary. Exclude any practice sets, questions, exercises, references to specific pages, or metadata from the summary. Focus entirely on providing a comprehensive, exhaustive, and information-rich summary that fully encapsulates all details of the original text. Do not shorten or simplify any conceptsâ€”retain the full depth and complexity of the content. If the summary exceeds 8192 tokens, stop at a natural breakpoint and indicate that additional content will follow in the next response. Ensure continuity between chunks without repeating previously covered information.{text}\",\n",
        "                  generation_config=genai.types.GenerationConfig(\n",
        "                  max_output_tokens=8192,\n",
        "                  temperature=0.3,\n",
        "                  top_p=0.9,\n",
        "                  top_k=40)\n",
        "            )\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Text summary error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_image_summary(self, image_path: str) -> Optional[str]:\n",
        "        try:\n",
        "            img = Image.open(image_path)\n",
        "            response = self.vision_model.generate_content(\n",
        "                [\"Describe this image concisely:\", img]\n",
        "            )\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Image summary error: {e}\")\n",
        "            return None\n",
        "\n",
        "class InstructorEmbeddingGenerator(EmbeddingGenerator):\n",
        "    def __init__(self, model_name: str = \"hkunlp/instructor-xl\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def generate_embedding(self, text: str) -> Optional[list]:\n",
        "        try:\n",
        "            instruction = \"Represent the summary for retrieval: \"\n",
        "            return self.model.encode([instruction + text])[0].tolist()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Embedding error: {e}\")\n",
        "            return None\n",
        "\n",
        "class SupabaseStorageClient:\n",
        "    def __init__(self, url: str, key: str):\n",
        "        self.client = create_client(url, key)\n",
        "\n",
        "    def insert_record(self, table: str, data: Dict[str, Any]) -> bool:\n",
        "        try:\n",
        "            self.client.table(table).insert(data).execute()\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Supabase insert error: {e}\")\n",
        "            return False\n",
        "\n",
        "# ---------------------------- Processors ----------------------------\n",
        "class TextContentProcessor(ContentProcessor):\n",
        "    def __init__(\n",
        "        self,\n",
        "        summary_generator: SummaryGenerator,\n",
        "        embedding_generator: EmbeddingGenerator,\n",
        "        storage_client: SupabaseStorageClient\n",
        "    ):\n",
        "        self.summary_generator = summary_generator\n",
        "        self.embedding_generator = embedding_generator\n",
        "        self.storage_client = storage_client\n",
        "\n",
        "    def process(self, base_path: str, course_id: str) -> None:\n",
        "        # Look for JSON files directly in the base directory\n",
        "        base_dir = Path(base_path)\n",
        "        if not base_dir.exists():\n",
        "            logger.warning(f\"Base directory not found: {base_dir}\")\n",
        "            return\n",
        "\n",
        "        # Process all JSON files in the base directory and subdirectories\n",
        "        for text_file in base_dir.glob(\"**/*.json\"):\n",
        "            try:\n",
        "                with open(text_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    # Handle both formats: direct content string or content in a field\n",
        "                    content = data if isinstance(data, str) else data.get('content', '')\n",
        "\n",
        "                    if not content:\n",
        "                        continue\n",
        "\n",
        "                    summary = self.summary_generator.generate_text_summary(content)\n",
        "                    if not summary:\n",
        "                        continue\n",
        "\n",
        "                    embedding = self.embedding_generator.generate_embedding(summary)\n",
        "                    if not embedding:\n",
        "                        continue\n",
        "\n",
        "                    # Generate a unique chunk ID using the file name or a UUID if needed\n",
        "                    chunk_id = text_file.stem\n",
        "\n",
        "                    record = {\n",
        "                        'course_id': course_id,\n",
        "                        # 'chunk_id': chunk_id,\n",
        "                        'content': content,\n",
        "                        'content_summary': summary,\n",
        "                        'summary_vector': embedding\n",
        "                    }\n",
        "\n",
        "                    if not self.storage_client.insert_record('material_text', record):\n",
        "                        logger.error(f\"Failed to insert text record: {chunk_id}\")\n",
        "                    else:\n",
        "                        logger.info(f\"Successfully processed text file: {text_file}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing text file {text_file}: {e}\")\n",
        "\n",
        "class ImageContentProcessor(ContentProcessor):\n",
        "    def __init__(\n",
        "        self,\n",
        "        summary_generator: SummaryGenerator,\n",
        "        embedding_generator: EmbeddingGenerator,\n",
        "        storage_client: SupabaseStorageClient\n",
        "    ):\n",
        "        self.summary_generator = summary_generator\n",
        "        self.embedding_generator = embedding_generator\n",
        "        self.storage_client = storage_client\n",
        "        # Expanded list of image formats\n",
        "        self.supported_formats = ['.png', '.jpg', '.jpeg', '.jb2', '.bmp', '.tiff', '.gif']\n",
        "\n",
        "    def process(self, base_path: str, course_id: str) -> None:\n",
        "        # Look for images directly in the base directory and subdirectories\n",
        "        base_dir = Path(base_path)\n",
        "        if not base_dir.exists():\n",
        "            logger.warning(f\"Base directory not found: {base_dir}\")\n",
        "            return\n",
        "\n",
        "        # Process all image files in base directory and subdirectories\n",
        "        for image_file in base_dir.glob(\"**/*\"):\n",
        "            if image_file.suffix.lower() not in self.supported_formats:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Try to open the image first to check if PIL can handle it\n",
        "                try:\n",
        "                    Image.open(str(image_file)).verify()  # Verify the image can be opened\n",
        "                except Exception as img_err:\n",
        "                    logger.warning(f\"Skipping unsupported image format: {image_file} - Error: {img_err}\")\n",
        "                    continue\n",
        "\n",
        "                summary = self.summary_generator.generate_image_summary(str(image_file))\n",
        "                if not summary:\n",
        "                    continue\n",
        "\n",
        "                embedding = self.embedding_generator.generate_embedding(summary)\n",
        "                if not embedding:\n",
        "                    continue\n",
        "\n",
        "                chunk_id = image_file.stem\n",
        "                record = {\n",
        "                    'course_id': course_id,\n",
        "                    # 'chunk_id': chunk_id,  # This line is commented out\n",
        "                    'content_path': str(image_file),\n",
        "                    'content_summary': summary,\n",
        "                    'summary_vector': embedding\n",
        "                }\n",
        "\n",
        "                if not self.storage_client.insert_record('material_image', record):\n",
        "                    logger.error(f\"Failed to insert image record: {chunk_id}\")\n",
        "                else:\n",
        "                    logger.info(f\"Successfully processed image: {image_file}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing image {image_file}: {e}\")\n",
        "\n",
        "# ---------------------------- Client Code ----------------------------\n",
        "class ContentProcessingPipeline:\n",
        "    def __init__(\n",
        "        self,\n",
        "        processors: List[ContentProcessor],\n",
        "        storage_client: SupabaseStorageClient\n",
        "    ):\n",
        "        self.processors = processors\n",
        "        self.storage_client = storage_client\n",
        "\n",
        "    def run(self, base_path: str, course_id: str) -> None:\n",
        "        if not Path(base_path).exists():\n",
        "            logger.error(f\"Base path does not exist: {base_path}\")\n",
        "            # Create the directory if it doesn't exist\n",
        "            Path(base_path).mkdir(parents=True, exist_ok=True)\n",
        "            logger.info(f\"Created directory: {base_path}\")\n",
        "\n",
        "        logger.info(f\"Starting processing for course: {course_id}\")\n",
        "        for processor in self.processors:\n",
        "            processor.process(base_path, course_id)\n",
        "        logger.info(f\"Completed processing for course: {course_id}\")\n",
        "\n",
        "# ---------------------------- Factory ----------------------------\n",
        "class ProcessorFactory:\n",
        "    @staticmethod\n",
        "    def create_pipeline(\n",
        "        supabase_url: str,\n",
        "        supabase_key: str,\n",
        "        gemini_api_key: str\n",
        "    ) -> ContentProcessingPipeline:\n",
        "        # Initialize dependencies\n",
        "        storage_client = SupabaseStorageClient(supabase_url, supabase_key)\n",
        "        summary_generator = GeminiSummaryGenerator(gemini_api_key)\n",
        "        embedding_generator = InstructorEmbeddingGenerator()\n",
        "\n",
        "        # Create processors\n",
        "        text_processor = TextContentProcessor(\n",
        "            summary_generator, embedding_generator, storage_client\n",
        "        )\n",
        "        image_processor = ImageContentProcessor(\n",
        "            summary_generator, embedding_generator, storage_client\n",
        "        )\n",
        "\n",
        "        return ContentProcessingPipeline(\n",
        "            processors=[text_processor,\n",
        "                        image_processor],\n",
        "            storage_client=storage_client\n",
        "        )\n",
        "\n",
        "# ---------------------------- Usage ----------------------------\n",
        "def main():\n",
        "    # Initialize with secrets (in production, use environment variables)\n",
        "    pipeline = ProcessorFactory.create_pipeline(\n",
        "        supabase_url=userdata.get('SUPABASE_URL'),\n",
        "        supabase_key=userdata.get('SUPABASE_ANON_KEY'),\n",
        "        gemini_api_key=userdata.get('GEMINI_API_KEY')\n",
        "    )\n",
        "\n",
        "    # Run processing - adjust the path to your actual content location\n",
        "    pipeline.run(\n",
        "        base_path=\"/content/Data/Book1/\",\n",
        "        course_id=\"2e51c619-03d2-4b36-90e5-cbd9e9ad0baa\"\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}