{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rwFyo8asXIR_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtCntx7zQKJE",
        "outputId": "515b0b1d-4915-4ce1-bdbe-e07e498ab31d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJX7lKwwQQ6p"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "def count_text_tokens(text):\n",
        "    \"\"\"Counts the number of tokens in a text string based on word count.\"\"\"\n",
        "    # A simple word split to approximate token count\n",
        "    words = text.split()\n",
        "    return len(words)\n",
        "\n",
        "def extract_text_from_pdf(pdf_file_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_file_path)\n",
        "        text = \"\"\n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            text += page.get_text(\"text\")  # Extract text from each page\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def count_pdf_tokens(pdf_file_path):\n",
        "    \"\"\"Counts tokens in a PDF file.\"\"\"\n",
        "    text = extract_text_from_pdf(pdf_file_path)\n",
        "    return count_text_tokens(text)\n",
        "\n",
        "def count_tokens_in_pdf_directory(directory_path):\n",
        "    \"\"\"Counts total tokens for all PDF files in a directory.\"\"\"\n",
        "    total_tokens = 0\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            print(f\"Processing file: {filename}\")\n",
        "            file_tokens = count_pdf_tokens(file_path)\n",
        "            total_tokens += file_tokens\n",
        "            print(f\"Tokens for {filename}: {file_tokens}\")\n",
        "\n",
        "    print(f\"Total tokens for all files in the directory: {total_tokens}\")\n",
        "    return total_tokens\n",
        "\n",
        "# Example usage:\n",
        "directory_path = \"/content/Data/\"\n",
        "total_tokens = count_tokens_in_pdf_directory(directory_path)+\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RLQq7-uxo5jk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "\n",
        "def count_text_tokens(text):\n",
        "    \"\"\"Counts the number of tokens in a text string based on word count.\"\"\"\n",
        "    # A simple word split to approximate token count\n",
        "    words = text.split()\n",
        "    return len(words)\n",
        "\n",
        "def estimate_base64_image_tokens(base64_string):\n",
        "    \"\"\"Estimates token cost for a base64-encoded image.\"\"\"\n",
        "    try:\n",
        "        decoded_image = base64.b64decode(base64_string)\n",
        "        image_size_bytes = len(decoded_image)\n",
        "        tokens = image_size_bytes / 50  # Adjust the divisor as needed\n",
        "        return int(tokens)\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding base64: {e}\")\n",
        "        return 0\n",
        "\n",
        "def count_json_tokens(json_file_path, include_images=False):\n",
        "    \"\"\"Counts tokens in a JSON file, optionally including base64 images.\"\"\"\n",
        "    try:\n",
        "        with open(json_file_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Convert the entire JSON content to a string and count tokens\n",
        "        json_string = json.dumps(data)\n",
        "        json_tokens = count_text_tokens(json_string)\n",
        "\n",
        "        image_tokens = 0\n",
        "        if include_images:\n",
        "            if isinstance(data, dict):\n",
        "                for value in data.values():\n",
        "                    if isinstance(value, str) and value.startswith(\"data:image/\"):\n",
        "                        image_tokens += estimate_base64_image_tokens(value.split(',')[1])\n",
        "            elif isinstance(data, list):\n",
        "                for item in data:\n",
        "                    if isinstance(item, dict):\n",
        "                        for value in item.values():\n",
        "                            if isinstance(value, str) and value.startswith(\"data:image/\"):\n",
        "                                image_tokens += estimate_base64_image_tokens(value.split(',')[1])\n",
        "\n",
        "        return json_tokens + image_tokens\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found: {json_file_path}\")\n",
        "        return 0\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Invalid JSON in file: {json_file_path}\")\n",
        "        return 0\n",
        "\n",
        "def count_tokens_in_directory(directory_path, include_images=False):\n",
        "    \"\"\"Counts total tokens for all JSON files in a directory.\"\"\"\n",
        "    total_tokens = 0\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if filename.endswith(\".json\"):\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            print(f\"Processing file: {filename}\")\n",
        "            file_tokens = count_json_tokens(file_path, include_images)\n",
        "            total_tokens += file_tokens\n",
        "            print(f\"Tokens for {filename}: {file_tokens}\")\n",
        "\n",
        "    print(f\"Total tokens for all files in the directory: {total_tokens}\")\n",
        "    return total_tokens\n",
        "\n",
        "# Example usage:\n",
        "directory_path = \"/content/Data/Book1/Text\"\n",
        "directory_path2 = \"/content/Data/Book1/Img\"\n",
        "\n",
        "include_images = True  # Set to False if you don't want to include base64 images\n",
        "\n",
        "total_tokens =  count_tokens_in_directory(directory_path, include_images) + count_tokens_in_directory(directory_path2, include_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OWd6EAWwwRxe",
        "outputId": "120511e5-4622-4544-84ff-fdf5f7927a01"
      },
      "outputs": [],
      "source": [
        "#START OF RAG GIG NO GAG\n",
        "!pip install langchain supabase sentence-transformers torch google-generativeai python-dotenv tqdm numpy multipart torch  langchain-community  InstructorEmbedding colorlog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C91NPka65Ab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from abc import ABC, abstractmethod\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Any\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from supabase import create_client, Client\n",
        "import websockets.sync\n",
        "import dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Configure the API key\n",
        "genai.configure(api_key=\"AIzaSyDNcnPNLS0Qg3Wjan8L-ok3V3pjb-4-1iQ\")\n",
        "\n",
        "# Instantiate the GenerativeModel\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')  # or 'gemini-pro-vision'\n",
        "\n",
        "# Generate content\n",
        "response = model.generate_content(\"\"\"hey can you generate an article just the tag with all the content in html and no css using the maximum output token limit that you can produce\n",
        "\n",
        "Try something with calculus and try to use all the output tokens\n",
        "\n",
        "show some formulas if possible\n",
        "\n",
        "also try writing some really complex formulas including integration and summation and stuff\n",
        "\n",
        "you know what infact show me in depth the laws of motion\n",
        "\n",
        "also only return the body tag and the content inside it and nothing else\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iucs-ErT67uN"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'userdata' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize with Colab secrets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m SUPABASE_URL \u001b[38;5;241m=\u001b[39m \u001b[43muserdata\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUPABASE_URL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m SUPABASE_KEY \u001b[38;5;241m=\u001b[39m userdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUPABASE_ANON_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m GOOGLE_API_KEY \u001b[38;5;241m=\u001b[39m userdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'userdata' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize with Colab secrets\n",
        "SUPABASE_URL = os.environ.get('SUPABASE_URL')\n",
        "SUPABASE_KEY = os.environ.get('SUPABASE_ANON_KEY')\n",
        "GOOGLE_API_KEY = os.environ.get('GEMINI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsPZFV-9yr6m",
        "outputId": "d583e6a0-cdc0-480a-ef2d-f4e5d8092c75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_10_chapter_7___transmission_media.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_10_chapter_7___transmission_media.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_10_chapter_7___transmission_media.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_10_chapter_7___transmission_media.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_27_chapter_24___congestion_control_and_quality_of_ser.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_27_chapter_24___congestion_control_and_quality_of_ser.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_27_chapter_24___congestion_control_and_quality_of_ser.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_27_chapter_24___congestion_control_and_quality_of_ser.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_17_chapter_14___wireless_lans.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_17_chapter_14___wireless_lans.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_17_chapter_14___wireless_lans.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_17_chapter_14___wireless_lans.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_34_chapter_31___network_security.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_34_chapter_31___network_security.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_34_chapter_31___network_security.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_34_chapter_31___network_security.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_35_chapter_32___security_in_the_internet__ipsec__ssl_.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_35_chapter_32___security_in_the_internet__ipsec__ssl_.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_35_chapter_32___security_in_the_internet__ipsec__ssl_.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_35_chapter_32___security_in_the_internet__ipsec__ssl_.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_13_chapter_10___error_detection_and_correction.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_13_chapter_10___error_detection_and_correction.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_13_chapter_10___error_detection_and_correction.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_13_chapter_10___error_detection_and_correction.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_47_index.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_47_index.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_47_index.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_47_index.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_24_chapter_21___network_layer__address_mapping__error.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_24_chapter_21___network_layer__address_mapping__error.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_24_chapter_21___network_layer__address_mapping__error.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_24_chapter_21___network_layer__address_mapping__error.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_30_chapter_27___www_and_http.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_30_chapter_27___www_and_http.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_30_chapter_27___www_and_http.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_30_chapter_27___www_and_http.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_40_appendix_e___telephone_history.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_40_appendix_e___telephone_history.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_40_appendix_e___telephone_history.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_40_appendix_e___telephone_history.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: index.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: index.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: index.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: index.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_7_chapter_4___digital_transmission.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_7_chapter_4___digital_transmission.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_7_chapter_4___digital_transmission.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_7_chapter_4___digital_transmission.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_38_appendix_c___mathematical_review.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_38_appendix_c___mathematical_review.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_38_appendix_c___mathematical_review.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_38_appendix_c___mathematical_review.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_3_preface.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_3_preface.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_3_preface.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_3_preface.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_22_chapter_19___network_layer__logical_addressing.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_22_chapter_19___network_layer__logical_addressing.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_22_chapter_19___network_layer__logical_addressing.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_22_chapter_19___network_layer__logical_addressing.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_44_acronyms.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_44_acronyms.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_44_acronyms.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_44_acronyms.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_19_chapter_16___wireless_wans__cellular_telephone_and.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_19_chapter_16___wireless_wans__cellular_telephone_and.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_19_chapter_16___wireless_wans__cellular_telephone_and.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_19_chapter_16___wireless_wans__cellular_telephone_and.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_29_chapter_26___remote_logging__electronic_mail__and_.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_29_chapter_26___remote_logging__electronic_mail__and_.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_29_chapter_26___remote_logging__electronic_mail__and_.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_29_chapter_26___remote_logging__electronic_mail__and_.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_33_chapter_30___cryptography.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_33_chapter_30___cryptography.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_33_chapter_30___cryptography.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_33_chapter_30___cryptography.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_14_chapter_11___data_link_control.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_14_chapter_11___data_link_control.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_14_chapter_11___data_link_control.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_14_chapter_11___data_link_control.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_1_copywrite.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_1_copywrite.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_1_copywrite.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_1_copywrite.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_37_appendix_b___numbering_systems.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_37_appendix_b___numbering_systems.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_37_appendix_b___numbering_systems.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_37_appendix_b___numbering_systems.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: Forozon_text_elements.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: Forozon_text_elements.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: Forozon_text_elements.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: Forozon_text_elements.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_20_chapter_17___sonet_sdh.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_20_chapter_17___sonet_sdh.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_20_chapter_17___sonet_sdh.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_20_chapter_17___sonet_sdh.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_25_chapter_22___network_layer_delivery__forwarding__a.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_25_chapter_22___network_layer_delivery__forwarding__a.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_25_chapter_22___network_layer_delivery__forwarding__a.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_25_chapter_22___network_layer_delivery__forwarding__a.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_32_chapter_29___multimedia.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_32_chapter_29___multimedia.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_32_chapter_29___multimedia.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_32_chapter_29___multimedia.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_43_appendix_h___udp_and_tcp_ports.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_43_appendix_h___udp_and_tcp_ports.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_43_appendix_h___udp_and_tcp_ports.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_43_appendix_h___udp_and_tcp_ports.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_42_appendix_g___rfc_s.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_42_appendix_g___rfc_s.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_42_appendix_g___rfc_s.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_42_appendix_g___rfc_s.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_26_chapter_23___process-la-process_delivery__udp__tcp.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_26_chapter_23___process-la-process_delivery__udp__tcp.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_26_chapter_23___process-la-process_delivery__udp__tcp.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_26_chapter_23___process-la-process_delivery__udp__tcp.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_4_chapter_1___introduction.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_4_chapter_1___introduction.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_4_chapter_1___introduction.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_4_chapter_1___introduction.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_45_glossary.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_45_glossary.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_45_glossary.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_45_glossary.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_15_chapter_12___multiple_access.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_15_chapter_12___multiple_access.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_15_chapter_12___multiple_access.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_15_chapter_12___multiple_access.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_41_appendix_f___contact_addresses.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_41_appendix_f___contact_addresses.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_41_appendix_f___contact_addresses.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_41_appendix_f___contact_addresses.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_8_chapter_5___analog_transmission.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_8_chapter_5___analog_transmission.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_8_chapter_5___analog_transmission.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_8_chapter_5___analog_transmission.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_28_chapter_25___domain_name_system.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_28_chapter_25___domain_name_system.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_28_chapter_25___domain_name_system.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_28_chapter_25___domain_name_system.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_12_chapter_9___using_telephone_and_cable_networks_for.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_12_chapter_9___using_telephone_and_cable_networks_for.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_12_chapter_9___using_telephone_and_cable_networks_for.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_12_chapter_9___using_telephone_and_cable_networks_for.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_9_chapter_6___bandwidth_utilization__multiplexing_an.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_9_chapter_6___bandwidth_utilization__multiplexing_an.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_9_chapter_6___bandwidth_utilization__multiplexing_an.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_9_chapter_6___bandwidth_utilization__multiplexing_an.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_39_appendix_d___8b_6tcode.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_39_appendix_d___8b_6tcode.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_39_appendix_d___8b_6tcode.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_39_appendix_d___8b_6tcode.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_16_chapter_13___wired_lans__ethernet.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_16_chapter_13___wired_lans__ethernet.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_16_chapter_13___wired_lans__ethernet.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_16_chapter_13___wired_lans__ethernet.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_21_chapter_18___virtual-circuit_networks__frame_relay.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_21_chapter_18___virtual-circuit_networks__frame_relay.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_21_chapter_18___virtual-circuit_networks__frame_relay.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_21_chapter_18___virtual-circuit_networks__frame_relay.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_5_chapter_2___network_models.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_5_chapter_2___network_models.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_5_chapter_2___network_models.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_5_chapter_2___network_models.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_46_references.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_46_references.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_46_references.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_46_references.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_23_chapter_20___network_layer__internet_protocol.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_23_chapter_20___network_layer__internet_protocol.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_23_chapter_20___network_layer__internet_protocol.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_23_chapter_20___network_layer__internet_protocol.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_18_chapter_15___connecting_lans__backbone_networks__a.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_18_chapter_15___connecting_lans__backbone_networks__a.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_18_chapter_15___connecting_lans__backbone_networks__a.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_18_chapter_15___connecting_lans__backbone_networks__a.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_31_chapter_28___network_management__snmp.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_31_chapter_28___network_management__snmp.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_31_chapter_28___network_management__snmp.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_31_chapter_28___network_management__snmp.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_6_chapter_3___data_and_signals.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_6_chapter_3___data_and_signals.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_6_chapter_3___data_and_signals.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_6_chapter_3___data_and_signals.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_2_contents.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_2_contents.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_2_contents.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_2_contents.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_11_chapter_8___switching.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_11_chapter_8___switching.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_11_chapter_8___switching.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_11_chapter_8___switching.json\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_36_appendix_a___unicode.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_36_appendix_a___unicode.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[34mLoaded text content: chapter_36_appendix_a___unicode.json\u001b[0m\n",
            "INFO:ContentLoader:Loaded text content: chapter_36_appendix_a___unicode.json\n",
            "ERROR:GeminiGenerator:Summary error: HTTPConnectionPool(host='localhost', port=43033): Read timed out. (read timeout=600.0)\n",
            "WARNING:AcademicSystem:Failed to process text chunk: chapter_10_chapter_7___transmission_media\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import colorlog\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    LOG_FORMAT = '%(log_color)s%(levelname)-8s%(reset)s %(blue)s%(message)s'\n",
        "    LOG_COLORS = {\n",
        "        'DEBUG': 'cyan',\n",
        "        'INFO': 'green',\n",
        "        'WARNING': 'yellow',\n",
        "        'ERROR': 'red',\n",
        "        'CRITICAL': 'red,bg_white',\n",
        "    }\n",
        "    CONTENT_TYPES = {\n",
        "        'text': ['.json'],\n",
        "        'images': ['.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.gif', '.jb2']\n",
        "    }\n",
        "\n",
        "# ---------------------------- Content Loaders ----------------------------\n",
        "class ContentLoader:\n",
        "    \"\"\"Loads pre-processed content from structured directories\"\"\"\n",
        "\n",
        "    def __init__(self, base_path: str):\n",
        "        self.base_path = Path(base_path)\n",
        "        self.logger = colorlog.getLogger(\"ContentLoader\")\n",
        "        self._setup_logging()\n",
        "\n",
        "    def _setup_logging(self):\n",
        "        handler = colorlog.StreamHandler()\n",
        "        handler.setFormatter(colorlog.ColoredFormatter(\n",
        "            Config.LOG_FORMAT,\n",
        "            log_colors=Config.LOG_COLORS\n",
        "        ))\n",
        "        self.logger.addHandler(handler)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "    def validate_structure(self) -> bool:\n",
        "        \"\"\"Check if directory structure is valid\"\"\"\n",
        "        required = {'text', 'images'}  # Using set for case-insensitive check\n",
        "        existing_dirs = {d.name.lower(): d for d in self.base_path.iterdir() if d.is_dir()}\n",
        "\n",
        "        missing = []\n",
        "        for req in required:\n",
        "            if req not in existing_dirs:\n",
        "                missing.append(req)\n",
        "\n",
        "        if missing:\n",
        "            self.logger.warning(f\"Missing directories: {', '.join(missing)}\")\n",
        "            return False\n",
        "\n",
        "        # Update paths to use actual directory names\n",
        "        self.text_dir = existing_dirs['text']\n",
        "        self.images_dir = existing_dirs['images']\n",
        "        return True\n",
        "\n",
        "    def load_text_content(self) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"Load all JSON text content from text directory\"\"\"\n",
        "        # text_dir = self.base_path / 'Text'\n",
        "        content = {}\n",
        "\n",
        "        for json_file in self.text_dir.glob('**/*.json'):\n",
        "            try:\n",
        "                with open(json_file, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    content[json_file.stem] = data\n",
        "                self.logger.info(f\"Loaded text content: {json_file.name}\")\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Failed to load {json_file}: {str(e)}\")\n",
        "\n",
        "        return content\n",
        "\n",
        "    def load_image_metadata(self) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"Load image metadata from images directory\"\"\"\n",
        "        # img_dir = self.base_path / 'Images'\n",
        "        images = {}\n",
        "\n",
        "        for img_file in self.images_dir.glob('**/*'):\n",
        "            if img_file.suffix.lower() in Config.CONTENT_TYPES['images']:\n",
        "                try:\n",
        "                    images[img_file.stem] = {\n",
        "                        'path': str(img_file),\n",
        "                        'format': img_file.suffix[1:].upper(),\n",
        "                        'size': img_file.stat().st_size\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Failed to process {img_file}: {str(e)}\")\n",
        "\n",
        "        return images\n",
        "\n",
        "# ---------------------------- AI Processors ----------------------------\n",
        "class TextProcessor:\n",
        "    \"\"\"Process text content from JSON files\"\"\"\n",
        "\n",
        "    def __init__(self, supabase: Client, embedder, summarizer):\n",
        "        self.supabase = supabase\n",
        "        self.embedder = embedder\n",
        "        self.summarizer = summarizer\n",
        "        self.logger = colorlog.getLogger(\"TextProcessor\")\n",
        "\n",
        "    def process_chunk(self, chunk_id: str, content: Dict, course_id: str) -> bool:\n",
        "        \"\"\"Process individual text chunk\"\"\"\n",
        "        try:\n",
        "            # Handle different JSON structures\n",
        "            text_content = content.get('content', '') if isinstance(content, dict) else str(content)\n",
        "\n",
        "            # Generate summary and embedding\n",
        "            summary = self.summarizer.generate_text_summary(text_content)\n",
        "            if not summary:\n",
        "                return False\n",
        "\n",
        "            embedding = self.embedder.encode(summary).tolist()\n",
        "            if not embedding:\n",
        "                return False\n",
        "\n",
        "            # Create database record\n",
        "            record = {\n",
        "                'course_id': course_id,\n",
        "                'chunk_id': str(uuid.uuid5(uuid.NAMESPACE_URL, chunk_id)),\n",
        "                'content': text_content,\n",
        "                'content_summary': summary,\n",
        "                'summary_vector': embedding\n",
        "            }\n",
        "\n",
        "            result = self.supabase.table('material_text').insert(record).execute()\n",
        "            return len(result.data) > 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to process chunk {chunk_id}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "class ImageProcessor:\n",
        "    \"\"\"Process image content\"\"\"\n",
        "\n",
        "    def __init__(self, supabase: Client, embedder, summarizer):\n",
        "        self.supabase = supabase\n",
        "        self.embedder = embedder\n",
        "        self.summarizer = summarizer\n",
        "        self.logger = colorlog.getLogger(\"ImageProcessor\")\n",
        "\n",
        "    def process_image(self, image_id: str, metadata: Dict, course_id: str) -> bool:\n",
        "        \"\"\"Process individual image\"\"\"\n",
        "        try:\n",
        "            # Validate image file\n",
        "            if not Path(metadata['path']).exists():\n",
        "                self.logger.warning(f\"Missing image file: {metadata['path']}\")\n",
        "                return False\n",
        "\n",
        "            # Generate summary and embedding\n",
        "            summary = self.summarizer.generate_image_summary(metadata['path'])\n",
        "            if not summary:\n",
        "                return False\n",
        "\n",
        "            embedding = self.embedder.encode(summary).tolist()\n",
        "            if not embedding:\n",
        "                return False\n",
        "\n",
        "            # Create database record\n",
        "            record = {\n",
        "                'course_id': course_id,\n",
        "                'chunk_id': str(uuid.uuid5(uuid.NAMESPACE_URL, image_id)),\n",
        "                'content_path': metadata['path'],\n",
        "                'content_summary': summary,\n",
        "                'summary_vector': embedding,\n",
        "                'metadata': {\n",
        "                    'format': metadata['format'],\n",
        "                    'size': metadata['size']\n",
        "                }\n",
        "            }\n",
        "\n",
        "            result = self.supabase.table('material_image').insert(record).execute()\n",
        "            return len(result.data) > 0\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to process image {image_id}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "# ---------------------------- Main System ----------------------------\n",
        "class AcademicContentSystem:\n",
        "    \"\"\"Main system for processing existing content\"\"\"\n",
        "\n",
        "    def __init__(self, supabase_url: str, supabase_key: str, gemini_api_key: str):\n",
        "        self.supabase = create_client(supabase_url, supabase_key)\n",
        "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.summarizer = GeminiSummaryGenerator(gemini_api_key)\n",
        "        self.logger = colorlog.getLogger(\"AcademicSystem\")\n",
        "\n",
        "    def process_content(self, base_path: str, course_id: str) -> bool:\n",
        "        \"\"\"Main processing method\"\"\"\n",
        "        loader = ContentLoader(base_path)\n",
        "\n",
        "        if not loader.validate_structure():\n",
        "            self.logger.error(\"Invalid content structure\")\n",
        "            return False\n",
        "\n",
        "        # Process text content\n",
        "        text_processor = TextProcessor(self.supabase, self.embedder, self.summarizer)\n",
        "        text_content = loader.load_text_content()\n",
        "\n",
        "        for chunk_id, content in text_content.items():\n",
        "            if not text_processor.process_chunk(chunk_id, content, course_id):\n",
        "                self.logger.warning(f\"Failed to process text chunk: {chunk_id}\")\n",
        "\n",
        "        # Process images\n",
        "        image_processor = ImageProcessor(self.supabase, self.embedder, self.summarizer)\n",
        "        image_metadata = loader.load_image_metadata()\n",
        "\n",
        "        for img_id, metadata in image_metadata.items():\n",
        "            if not image_processor.process_image(img_id, metadata, course_id):\n",
        "                self.logger.warning(f\"Failed to process image: {img_id}\")\n",
        "\n",
        "        self.logger.info(f\"Completed processing for course: {course_id}\")\n",
        "        return True\n",
        "\n",
        "class GeminiSummaryGenerator:\n",
        "    \"\"\"Modified summary generator for existing content\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "        self.logger = colorlog.getLogger(\"GeminiGenerator\")\n",
        "\n",
        "    def generate_text_summary(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Generate text summary with error handling\"\"\"\n",
        "        try:\n",
        "            response = self.model.generate_content(\n",
        "                f\"Generate comprehensive summary: {text}\",\n",
        "                generation_config=genai.types.GenerationConfig(\n",
        "                    max_output_tokens=2000,\n",
        "                    temperature=0.3\n",
        "                )\n",
        "            )\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Summary error: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def generate_image_summary(self, image_path: str) -> Optional[str]:\n",
        "        \"\"\"Generate image summary with validation\"\"\"\n",
        "        try:\n",
        "            img = Image.open(image_path)\n",
        "            response = self.model.generate_content([\"Describe in detail:\", img])\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Image processing error: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "# ---------------------------- Usage ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize with environment variables\n",
        "    system = AcademicContentSystem(\n",
        "       supabase_url=userdata.get('SUPABASE_URL'),\n",
        "        supabase_key=userdata.get('SUPABASE_ANON_KEY'),\n",
        "        gemini_api_key=userdata.get('GEMINI_API_KEY')\n",
        "    )\n",
        "\n",
        "    # Process content from existing directories\n",
        "    system.process_content(\n",
        "        base_path=\"/content/Data\",\n",
        "        course_id=\"b8c30d7c-56c5-4439-a235-8a756471278f\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DtBby7KO-6XQ",
        "outputId": "573cc5c2-5208-46c1-b835-758d46de448c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "ls: invalid option -- 'e'\n",
            "Try 'ls --help' for more information.\n",
            "Usage: ls [OPTION]... [FILE]...\n",
            "List information about the FILEs (the current directory by default).\n",
            "Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.\n",
            "\n",
            "Mandatory arguments to long options are mandatory for short options too.\n",
            "  -a, --all                  do not ignore entries starting with .\n",
            "  -A, --almost-all           do not list implied . and ..\n",
            "      --author               with -l, print the author of each file\n",
            "  -b, --escape               print C-style escapes for nongraphic characters\n",
            "      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;\n",
            "                               e.g., '--block-size=M'; see SIZE format below\n",
            "  -B, --ignore-backups       do not list implied entries ending with ~\n",
            "  -c                         with -lt: sort by, and show, ctime (time of last\n",
            "                               modification of file status information);\n",
            "                               with -l: show ctime and sort by name;\n",
            "                               otherwise: sort by ctime, newest first\n",
            "  -C                         list entries by columns\n",
            "      --color[=WHEN]         colorize the output; WHEN can be 'always' (default\n",
            "                               if omitted), 'auto', or 'never'; more info below\n",
            "  -d, --directory            list directories themselves, not their contents\n",
            "  -D, --dired                generate output designed for Emacs' dired mode\n",
            "  -f                         do not sort, enable -aU, disable -ls --color\n",
            "  -F, --classify             append indicator (one of */=>@|) to entries\n",
            "      --file-type            likewise, except do not append '*'\n",
            "      --format=WORD          across -x, commas -m, horizontal -x, long -l,\n",
            "                               single-column -1, verbose -l, vertical -C\n",
            "      --full-time            like -l --time-style=full-iso\n",
            "  -g                         like -l, but do not list owner\n",
            "      --group-directories-first\n",
            "                             group directories before files;\n",
            "                               can be augmented with a --sort option, but any\n",
            "                               use of --sort=none (-U) disables grouping\n",
            "  -G, --no-group             in a long listing, don't print group names\n",
            "  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.\n",
            "      --si                   likewise, but use powers of 1000 not 1024\n",
            "  -H, --dereference-command-line\n",
            "                             follow symbolic links listed on the command line\n",
            "      --dereference-command-line-symlink-to-dir\n",
            "                             follow each command line symbolic link\n",
            "                               that points to a directory\n",
            "      --hide=PATTERN         do not list implied entries matching shell PATTERN\n",
            "                               (overridden by -a or -A)\n",
            "      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'\n",
            "                               (default if omitted), 'auto', or 'never'\n",
            "      --indicator-style=WORD  append indicator with style WORD to entry names:\n",
            "                               none (default), slash (-p),\n",
            "                               file-type (--file-type), classify (-F)\n",
            "  -i, --inode                print the index number of each file\n",
            "  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN\n",
            "  -k, --kibibytes            default to 1024-byte blocks for disk usage;\n",
            "                               used only with -s and per directory totals\n",
            "  -l                         use a long listing format\n",
            "  -L, --dereference          when showing file information for a symbolic\n",
            "                               link, show information for the file the link\n",
            "                               references rather than for the link itself\n",
            "  -m                         fill width with a comma separated list of entries\n",
            "  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs\n",
            "  -N, --literal              print entry names without quoting\n",
            "  -o                         like -l, but do not list group information\n",
            "  -p, --indicator-style=slash\n",
            "                             append / indicator to directories\n",
            "  -q, --hide-control-chars   print ? instead of nongraphic characters\n",
            "      --show-control-chars   show nongraphic characters as-is (the default,\n",
            "                               unless program is 'ls' and output is a terminal)\n",
            "  -Q, --quote-name           enclose entry names in double quotes\n",
            "      --quoting-style=WORD   use quoting style WORD for entry names:\n",
            "                               literal, locale, shell, shell-always,\n",
            "                               shell-escape, shell-escape-always, c, escape\n",
            "                               (overrides QUOTING_STYLE environment variable)\n",
            "  -r, --reverse              reverse order while sorting\n",
            "  -R, --recursive            list subdirectories recursively\n",
            "  -s, --size                 print the allocated size of each file, in blocks\n",
            "  -S                         sort by file size, largest first\n",
            "      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),\n",
            "                               time (-t), version (-v), extension (-X)\n",
            "      --time=WORD            change the default of using modification times;\n",
            "                               access time (-u): atime, access, use;\n",
            "                               change time (-c): ctime, status;\n",
            "                               birth time: birth, creation;\n",
            "                             with -l, WORD determines which time to show;\n",
            "                             with --sort=time, sort by WORD (newest first)\n",
            "      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below\n",
            "  -t                         sort by time, newest first; see --time\n",
            "  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8\n",
            "  -u                         with -lt: sort by, and show, access time;\n",
            "                               with -l: show access time and sort by name;\n",
            "                               otherwise: sort by access time, newest first\n",
            "  -U                         do not sort; list entries in directory order\n",
            "  -v                         natural sort of (version) numbers within text\n",
            "  -w, --width=COLS           set output width to COLS.  0 means no limit\n",
            "  -x                         list entries by lines instead of by columns\n",
            "  -X                         sort alphabetically by entry extension\n",
            "  -Z, --context              print any security context of each file\n",
            "  -1                         list one file per line.  Avoid '\\n' with -q or -b\n",
            "      --help     display this help and exit\n",
            "      --version  output version information and exit\n",
            "\n",
            "The SIZE argument is an integer and optional unit (example: 10K is 10*1024).\n",
            "Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).\n",
            "Binary prefixes can be used, too: KiB=K, MiB=M, and so on.\n",
            "\n",
            "The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.\n",
            "FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,\n",
            "then FORMAT1 applies to non-recent files and FORMAT2 to recent files.\n",
            "TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.\n",
            "Also the TIME_STYLE environment variable sets the default style to use.\n",
            "\n",
            "Using color to distinguish file types is disabled both by default and\n",
            "with --color=never.  With --color=auto, ls emits color codes only when\n",
            "standard output is connected to a terminal.  The LS_COLORS environment\n",
            "variable can change the settings.  Use the dircolors command to set it.\n",
            "\n",
            "Exit status:\n",
            " 0  if OK,\n",
            " 1  if minor problems (e.g., cannot access subdirectory),\n",
            " 2  if serious trouble (e.g., cannot access command-line argument).\n",
            "\n",
            "GNU coreutils online help: <https://www.gnu.org/software/coreutils/>\n",
            "Full documentation <https://www.gnu.org/software/coreutils/ls>\n",
            "or available locally via: info '(coreutils) ls invocation'\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "FmCOtEPVWZ6d",
        "outputId": "d185ffe0-8441-4191-9cc6-fd67fc1e6d5b"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-11-e0596874629b>, line 6)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-e0596874629b>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    model_repo = \"aryan083/AI-Face-Tracker\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-pt\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-pt\")\n",
        "   # Set the Hugging Face model repository link\n",
        "model_repo = \"aryan083/AI-Face-Tracker\"\n",
        "\n",
        "# Load the model from Hugging Face\n",
        "sam = SamModel.from_pretrained(model_repo)\n",
        "\n",
        "# Initialize the predictor\n",
        "predictor = SamPredictor(sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "31-qAY7UY-13",
        "outputId": "e246a1cc-a7a1-48a3-be44-e5f80665cd14"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "The checkpoint you are trying to load has model type `gemma3_text` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m                 \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'gemma3_text'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-37b33d6f3aef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the model from Hugging Face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ukhirani/gemma-thesaviour\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m                 \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1074\u001b[0m                     \u001b[0;34mf\"The checkpoint you are trying to load has model type `{config_dict['model_type']}` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     \u001b[0;34m\"but Transformers does not recognize this architecture. This could be because of an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The checkpoint you are trying to load has model type `gemma3_text` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Set the Hugging Face model repository link\n",
        "# model_repo = \"ukhirani/gemma-thesaviour\"\n",
        "\n",
        "# Load the model from Hugging Face\n",
        "model = AutoModel.from_pretrained(\"ukhirani/gemma-thesaviour\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ukhirani/gemma-thesaviour\")\n",
        "\n",
        "# Test the model with sample input\n",
        "text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")  # Convert input text to tensors\n",
        "\n",
        "# Perform inference\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
